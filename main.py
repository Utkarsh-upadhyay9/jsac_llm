# -*- coding: utf-8 -*-
"""Jsac_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KunetvKNmL4L0mIzrYw-zC8Hcuc4RsjW
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt
import os
from transformers import DistilBertTokenizer, DistilBertModel

print("--- JSAC Actor Network Comparison Script ---")

# --- 1. Setup and Environment Definition ---
# Create a directory for plots if it doesn't exist
if not os.path.exists('plots'):
    os.makedirs('plots')
    print("Created 'plots' directory for saving results.")

# Set seed for reproducibility
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

# System Parameters
N, M = 32, 16 #######################################from 4,2
sigma2 = 1e-14 ############################################from 0.1
P_max = 1
snr_min = 1e-8
omega = 0.5 #####################################from 0.5
beta, B = 0.8, 1.0 #########################################################from 0.8,1.0

# --- IEEE-Based Multi-Link Pathloss Initialization ---
V = 3  # Number of vehicular users
V_users = V

I = 1  # One IAB node (single-cell setup for now)
lambda_c = 0.003  # Carrier wavelength (100 GHz)
c1, c2 = 11.95, 0.136
xi_los, xi_nlos = 2.0, 3.5
alpha = 2.5
h0 = 10  # height difference

# Fixed user and node positions
vu_pos = 20  # representative VU
iab_pos = 40
ris_pos = 30
donor_pos = 100 ######################################################from 100
eve_pos = 70 ########################################################from 70

# Equation (4) - Implements the average path loss model PL_t,r.
# This function combines the LoS and NLoS probabilities from Equation (3).
def compute_pathloss(d, alpha=2.5, xi_los=2.0, xi_nlos=3.5, lambda_c=0.003, h0=10, c1=11.95, c2=0.136):
    rho0 = (4 * np.pi / lambda_c) ** 2 # Reference path loss, part of Equation (2)
    angle_deg = np.degrees(np.arctan(h0 / d))
    # Equation (3) - Probability of Line-of-Sight (LoS)
    chi_los = 1 / (1 + c2 * np.exp(-c1 * (angle_deg - c2)))
    chi_nlos = 1 - chi_los
    attenuation = chi_los * xi_los + chi_nlos * xi_nlos
    # Equation (2) - General path loss model
    return rho0 * attenuation * d**alpha

# Compute pathlosses for various links
pl_br = compute_pathloss(abs(iab_pos - ris_pos))  # BS-to-RIS
pl_ru = compute_pathloss(abs(ris_pos - vu_pos))   # RIS-to-User
pl_be = compute_pathloss(abs(iab_pos - eve_pos))  # BS-to-Eve
pl_e  = compute_pathloss(abs(ris_pos - eve_pos))  # RIS-to-Eve

# Channels with scaled pathloss
H_be = (np.random.randn(N, M) + 1j*np.random.randn(N, M)) / np.sqrt(2) * np.sqrt(1 / pl_be)
h_e  = (np.random.randn(1, N) + 1j*np.random.randn(1, N)) / np.sqrt(2) * np.sqrt(1 / pl_e)


# --- RIS-to-VU Channel: LoS only spatial signature ---
# Equation (5) - Channel gain vector from RIS to VU (h_s,v).
# Models the spatial signature of the LoS path.
phi_sv = 1  # Cosine of AoA from VU to RIS
d_sv = abs(ris_pos - vu_pos)
pl_sv = compute_pathloss(d_sv)
h_ru = np.array([np.exp(-1j * 2 * np.pi / lambda_c * n * d_sv * phi_sv) for n in range(N)])
h_ru = (1 / np.sqrt(pl_sv)) * h_ru.reshape(1, -1)  # Shape: (1, N)

kappa = 10  # Rician factor (LoS power / NLoS power), typical range: 0 (Rayleigh) to 20+

# --- IAB-to-RIS Channel: Rician fading with LoS + NLoS ---
# Equation (6) & (7) - Channel gain matrix from IAB to RIS (H_i,s).
# This models the Rician fading channel, including both LoS and NLoS components.
phi_is = 1  # Cosine of AoD from IAB to RIS
d_is = abs(iab_pos - ris_pos)
pl_is = compute_pathloss(d_is)
H_br = np.zeros((N, M), dtype=complex)
for m in range(M):
    # Equation (7) - LoS component of the channel from the m-th antenna.
    h_los = np.array([np.exp(-1j * 2 * np.pi / lambda_c * n * d_is * phi_is) for n in range(N)])
    # NLoS component (scattered)
    h_nlos = (np.random.randn(N) + 1j * np.random.randn(N)) / np.sqrt(2)
    # Rician fading vector for m-th column
    H_br[:, m] = np.sqrt(kappa / (1 + kappa)) * h_los + np.sqrt(1 / (1 + kappa)) * h_nlos
H_br *= (1 / np.sqrt(pl_is))

# --- IAB Donor to IAB node backhaul link ---
# Equation (8) - Channel gain for the backhaul link (h_D,i).
d_di = abs(donor_pos - iab_pos)
pl_di = compute_pathloss(d_di)
h_backhaul = (np.random.randn() + 1j * np.random.randn()) / np.sqrt(pl_di)


def _scalar(x): return float(np.real(x).ravel()[0])

# SNR and Rate Computations
# Equation (14) - Signal-to-Interference-plus-Noise Ratio (SINR) at VU v (Γ_v).
# Calculates the SINR for a vehicular user, considering the constructively aligned signals.
def compute_snr_delayed(phases, W_tau, W_o, v_idx=0, h_direct=None):
    if h_direct is None:
        h_direct = ((np.random.randn(M, 1) + 1j * np.random.randn(M, 1)) / np.sqrt(2)).T  # Shape: (1, M)

    # Equation (1) - RIS diagonal reflection matrix (Θ).
    theta = np.exp(1j * phases)
    Theta = np.diag(theta)
    h_tilde = h_ru @ Theta @ H_br  # RIS-assisted channel

    W_tau_v = W_tau[:, v_idx].reshape(-1, 1)
    W_o_v = W_o[:, v_idx].reshape(-1, 1)

    # Diagnostic only — remove or comment in production
    # This check relates to the TZF conditions in Equation (12).
    if not np.allclose(h_direct @ W_o, 0, atol=1e-3):
        print("Warning: TZF on h_direct not perfectly enforced.")

    signal = np.abs(h_direct @ W_tau_v + h_tilde @ W_o_v) ** 2
    interference = 0
    V = W_tau.shape[1]
    for j in range(V):
        if j == v_idx:
            continue
        W_tau_j = W_tau[:, j].reshape(-1, 1)
        W_o_j = W_o[:, j].reshape(-1, 1)
        interference += np.abs(h_direct @ W_tau_j + h_tilde @ W_o_j) ** 2

    sinr = signal / (interference + sigma2)
    return _scalar(sinr)


# Equation (16) & (17) - Worst-case SINR at eavesdropper (Γ_e^(c)).
# This function calculates the maximum possible SINR the eavesdropper could achieve.
def compute_eve_sinr_maxcase(phases, W_tau, W_o):
    """Worst-case SINR at eavesdropper as per Gamma_e^(c)"""
    # Equation (1) - RIS diagonal reflection matrix (Θ).
    theta = np.exp(1j * phases)
    Theta = np.diag(theta)

    h_direct_e = (np.random.randn(1, M) + 1j * np.random.randn(1, M)) / np.sqrt(2)
    h_ris_e = h_e @ Theta @ H_be
    V = W_tau.shape[1]

    num = 0
    for q in [W_tau, W_o]:
        for v in range(V):
            w_v = q[:, v].reshape(-1, 1)
            num += np.abs(h_direct_e @ w_v)**2
            num += np.abs(h_ris_e @ w_v)**2

    max_term = 0
    for q in [W_tau, W_o]:
        for v in range(V):
            w_v = q[:, v].reshape(-1, 1)
            term = np.abs(h_direct_e @ w_v)**2 + np.abs(h_ris_e @ w_v)**2
            max_term = max(max_term, term)

    gamma_e = (num + sigma2) / (max_term + 1e-6) - 1
    return _scalar(1 / gamma_e)



def compute_eve_snr(phases, w):
    theta = np.exp(1j * phases)
    Theta = np.diag(theta)
    eff_e = h_e @ Theta @ H_be @ w
    snr_e = np.abs(eff_e)**2 / sigma2
    return _scalar(snr_e)

# Equation (19) - SNR for Target Echo at IAB Node (Γ_i^(s)).
# Computes the sensing SNR based on the echo received from the target.
def compute_sensing_snr(W_tau, W_o, phases, sigma_i=0.1):
    """Compute radar-style sensing SNR at IAB node based on echo from target g"""

    # RIS-assisted channel to target (LoS model)
    phi_sg = 1
    d_sg = abs(ris_pos - 60)  # assume target g at 60m
    pl_sg = compute_pathloss(d_sg)
    h_sg = np.array([np.exp(-1j * 2 * np.pi / lambda_c * n * d_sg * phi_sg) for n in range(N)]).reshape(1, -1)
    h_sg = h_sg / np.sqrt(pl_sg)

    # IAB-to-target direct path
    f_direct = (np.random.randn(M, 1) + 1j * np.random.randn(M, 1)) / np.sqrt(2)

    # RIS-assisted path
    # Equation (1) - RIS diagonal reflection matrix (Θ).
    theta = np.exp(1j * phases)
    Theta = np.diag(theta)
    H_is = H_br  # reuse from earlier
    f_ris = (h_sg @ Theta @ H_is).reshape(M, 1)

    # Composite channel F_i_g = f f^H + f_tilde f_tilde^H (defined in text before Eq. 19)
    F1 = f_direct @ f_direct.conj().T
    F2 = f_ris @ f_ris.conj().T
    F_ig = F1 + F2

    # Radar receive vector (unit-norm) - corresponds to 'u' in Eq. 19
    u = np.random.randn(M, 1) + 1j * np.random.randn(M, 1)
    # Constraint C8: ||u||^2 = 1
    u = u / np.linalg.norm(u)

    # Composite transmit power matrix
    P_tx = W_tau @ W_tau.conj().T + W_o @ W_o.conj().T

    # SNR calculation as per Equation (19)
    snr_sense = (u.conj().T @ F_ig @ P_tx @ F_ig.conj().T @ u).real / sigma_i**2
    return _scalar(snr_sense)


# --- 2. Prompt Engineering for LLM ---
def create_prompt(state_np, reward_info=None):
    """Converts the numerical state vector into a descriptive text prompt for the LLM."""
    ris_phases = state_np[:N]
    bs_beamforming = state_np[N:]
    prompt = (f"Task: Optimize RIS and beamforming to balance secrecy and sensing rates. "
              f"Objective: Maximize weighted reward. "
              f"Current RIS phases are {np.round(ris_phases, 2)}. "
              f"Current BS beamforming is {np.round(bs_beamforming, 2)}. ")
    if reward_info:
        prompt += (f"Last secrecy rate was {reward_info['secrecy']:.2f}. "
                   f"Last sensing rate was {reward_info['sensing']:.2f}.")
    return prompt

# --- 3. Replay Buffers ---
class ReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    def push(self, s, a, r, s2):
        self.buffer.append((s, a, r, s2))
    def sample(self, batch_size):
        samples = random.sample(self.buffer, batch_size)
        s, a, r, s2 = map(np.array, zip(*samples))
        return map(torch.FloatTensor, (s, a, r, s2))
    def __len__(self): return len(self.buffer)

class TextReplayBuffer:
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
    def push(self, experience_tuple):
        self.buffer.append(experience_tuple)
    def sample(self, batch_size, tokenizer):
        samples = random.sample(self.buffer, batch_size)
        prompts, actions, rewards, next_prompts, states_np, next_states_np = zip(*samples)
        inputs = tokenizer(list(prompts), return_tensors='pt', padding=True, truncation=True, max_length=128)
        next_inputs = tokenizer(list(next_prompts), return_tensors='pt', padding=True, truncation=True, max_length=128)
        actions_tensor = torch.FloatTensor(np.array(actions))
        rewards_tensor = torch.FloatTensor(np.array(rewards))
        states_tensor = torch.FloatTensor(np.array(states_np))
        next_states_tensor = torch.FloatTensor(np.array(next_states_np))
        return (inputs, actions_tensor, rewards_tensor, next_inputs, states_tensor, next_states_tensor)
    def __len__(self): return len(self.buffer)

# --- 4. Actor and Critic Network Architectures ---
class ActorMLP(nn.Module):
    def __init__(self, s_dim, a_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(s_dim, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, a_dim), nn.Tanh())
    def forward(self, s): return self.net(s)

class ActorLLM(nn.Module):
    def __init__(self, action_dim, llm_model_name='distilbert-base-uncased'):
        super().__init__()
        self.llm = DistilBertModel.from_pretrained(llm_model_name)
        self.fc1 = nn.Linear(self.llm.config.dim, 128)
        self.fc2 = nn.Linear(128, action_dim)
    def forward(self, input_ids, attention_mask):
        with torch.no_grad():
            outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]
        x = torch.relu(self.fc1(cls_output))
        return torch.tanh(self.fc2(x))

class ActorHybrid(nn.Module):
    def __init__(self, state_dim, action_dim, llm_model_name='distilbert-base-uncased'):
        super().__init__()
        self.llm = DistilBertModel.from_pretrained(llm_model_name)
        self.llm_fc = nn.Linear(self.llm.config.dim, 64)
        self.cnn_fc = nn.Linear(state_dim, 64) # Using simple Linear for numerical part
        self.combine_fc1 = nn.Linear(128, 128)
        self.output_fc = nn.Linear(128, action_dim)
    def forward(self, state, input_ids, attention_mask):
        with torch.no_grad():
            llm_out = self.llm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]
        llm_features = torch.relu(self.llm_fc(llm_out))
        numeric_features = torch.relu(self.cnn_fc(state))
        combined = torch.cat((llm_features, numeric_features), dim=1)
        x = torch.relu(self.combine_fc1(combined))
        return torch.tanh(self.output_fc(x))

class Critic(nn.Module):
    def __init__(self, s_dim, a_dim):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(s_dim + a_dim, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 1))
    def forward(self, s, a): return self.net(torch.cat([s, a], dim=-1))

# --- 5. DDPG Agent Class ---
class DDPGAgent:
    def __init__(self, name, actor_class, critic_class, state_dim, action_dim, is_text_based=False, is_hybrid=False):
        self.name = name
        self.is_text_based = is_text_based
        self.is_hybrid = is_hybrid

        actor_args = (action_dim,) if is_text_based else (state_dim, action_dim)
        if is_hybrid: actor_args = (state_dim, action_dim)

        self.actor = actor_class(*actor_args)
        self.critic = critic_class(state_dim, action_dim)
        self.target_actor = actor_class(*actor_args)
        self.target_critic = critic_class(state_dim, action_dim)

        self.target_actor.load_state_dict(self.actor.state_dict())
        self.target_critic.load_state_dict(self.critic.state_dict())

        self.opt_actor = optim.Adam(self.actor.parameters(), lr=1e-4)
        self.opt_critic = optim.Adam(self.critic.parameters(), lr=1e-3)

        self.replay_buffer = TextReplayBuffer() if is_text_based or is_hybrid else ReplayBuffer()
        self.reward_history = []

    def update(self, batch_size, gamma, tau, tokenizer=None):
        if len(self.replay_buffer) < batch_size: return

        if self.is_text_based or self.is_hybrid:
            inputs, actions, rewards, next_inputs, states, next_states = self.replay_buffer.sample(batch_size, tokenizer)
        else:
            states, actions, rewards, next_states = self.replay_buffer.sample(batch_size)

        with torch.no_grad():
            if self.is_text_based:
                next_actions = self.target_actor(next_inputs['input_ids'], next_inputs['attention_mask'])
            elif self.is_hybrid:
                next_actions = self.target_actor(next_states, next_inputs['input_ids'], next_inputs['attention_mask'])
            else:
                next_actions = self.target_actor(next_states)
            target_q = rewards + gamma * self.target_critic(next_states, next_actions)

        q = self.critic(states, actions)
        critic_loss = nn.MSELoss()(q, target_q)
        self.opt_critic.zero_grad(); critic_loss.backward(); self.opt_critic.step()

        if self.is_text_based:
            actor_actions = self.actor(inputs['input_ids'], inputs['attention_mask'])
        elif self.is_hybrid:
            actor_actions = self.actor(states, inputs['input_ids'], inputs['attention_mask'])
        else:
            actor_actions = self.actor(states)

        actor_loss = -self.critic(states, actor_actions).mean()
        self.opt_actor.zero_grad(); actor_loss.backward(); self.opt_actor.step()

        for tp, p in zip(self.target_actor.parameters(), self.actor.parameters()):
            tp.data.copy_(tau * p.data + (1.0 - tau) * p.data)
        for tp, p in zip(self.target_critic.parameters(), self.critic.parameters()):
            tp.data.copy_(tau * p.data + (1.0 - tau) * p.data)

# --- 6. Training ---
# Hyperparameters
state_dim = N + M
action_dim = N + M
episodes = 1000
batch_size = 64
gamma = 0.99
tau = 0.005
noise_std = 0.5
noise_decay = 0.999
min_noise_std = 0.05
llm_model_name = 'distilbert-base-uncased'

# Initialize Tokenizer and Agents
print("Initializing tokenizer and agents...")
tokenizer = DistilBertTokenizer.from_pretrained(llm_model_name)
agents = {
    "MLP": DDPGAgent("MLP", ActorMLP, Critic, state_dim, action_dim),
    "LLM": DDPGAgent("LLM", ActorLLM, Critic, state_dim, action_dim, is_text_based=True),
    "Hybrid": DDPGAgent("Hybrid", ActorHybrid, Critic, state_dim, action_dim, is_hybrid=True)
}
print("Initialization complete.")

# Training Loop
print(f"Starting training for {episodes} episodes...")
for ep in range(episodes):

    ris_phases = np.random.uniform(0, 2*np.pi, N)
    bs_w = np.random.randn(M)
    bs_w = bs_w / np.linalg.norm(bs_w) * np.sqrt(P_max)
    state_np = np.concatenate([ris_phases, bs_w])

    state_tensor = torch.FloatTensor(state_np).unsqueeze(0)

    prompt = create_prompt(state_np)
    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=128)

    for name, agent in agents.items():
        with torch.no_grad():
            if agent.is_text_based:
                action = agent.actor(inputs['input_ids'], inputs['attention_mask']).numpy()[0]
            elif agent.is_hybrid:
                action = agent.actor(state_tensor, inputs['input_ids'], inputs['attention_mask']).numpy()[0]
            else:
                action = agent.actor(state_tensor).numpy()[0]

        noisy_action = action + np.random.normal(0, noise_std, action_dim)
        # Constraint C2: |θ_n| = 1. The action output is mapped to a valid phase in [0, 2*pi].
        ris_action = np.mod((noisy_action[:N] + 1) / 2 * 2 * np.pi, 2 * np.pi)
        bs_raw = (noisy_action[N:] + 1) / 2 * 2 - 1
        bs_action = bs_raw / np.linalg.norm(bs_raw) * np.sqrt(P_max)


        # Zero-Forcing Beamforming. This implements the concept of TZF and SZF.
        # Equation (12): TZF conditions (h_tilde * W = 0 and h * W = 0).
        # Constraint C3 & C4: The pseudo-inverse is used to find beamforming vectors W
        # that are orthogonal to the undesired channels, forcing interference to zero.
        H_users = np.random.randn(V_users, M) + 1j * np.random.randn(V_users, M)  # Shape: (V, M)
        H_users = H_users.T  # Now shape: (M, V)

        W_tau = np.linalg.pinv(H_users)  # Shape: (V, M) ZF approximation
        W_tau = W_tau.T  # Final shape: (M, V)
        W_o = np.copy(W_tau)  # (M, V)


        # Constraint C1: Tr(W_tau*W_tau^H + W_o*W_o^H) <= P_max
        # Normalize total power to meet the maximum power constraint.
        P_tx_total = np.trace(W_tau @ W_tau.conj().T + W_o @ W_o.conj().T).real
        if P_tx_total > P_max:
            scale = np.sqrt(P_max / P_tx_total)
            W_tau *= scale
            W_o *= scale


        snr_eve = compute_eve_sinr_maxcase(ris_action, W_tau, W_o)

        snr_comm = compute_snr_delayed(ris_action, W_tau, W_o, v_idx=0)
        snr_sense = compute_sensing_snr(W_tau, W_o, ris_action)

        secrecy_rate = 0
        sensing_rate = 0

        gamma_req = 0.001  # Required SINR in linear scale (~10 dB)
        gamma_s_min = 2  # Minimum sensing SNR

        # Constraint C5: Γ_v >= Γ_req
        # The communication rate is only calculated if the user's SNR meets the requirement.
        if snr_comm >= gamma_req:  # with backhaul
            # Equation (20) - Achievable communication rate at VU v (R_v).
            R_v = beta * B * np.log2(1 + max(snr_comm, snr_min))
            R_e = beta * B * np.log2(1 + max(snr_eve, snr_min))
            # Capacity of the donor-to-IAB backhaul link.
            C_D_i = beta * B * np.log2(1 + max(np.abs(h_backhaul)**2 / sigma2, snr_min))
            total_Rv = R_v * V
            # Equation (21) - Achievable communication rate for VU v in the backhaul channel (R_D,v).
            R_D_v = R_v / total_Rv * C_D_i
            # Equation (22) - End-to-End (E2E) achievable communication rate for VU v (R_E2E,v).
            R_E2E_v = (R_v * R_D_v) / (R_v + R_D_v + 1e-2)

            # Equation (23) - Communication secrecy rate (S^(c)).
            # This is also related to Constraint C9: S^(c) > 0.
            secrecy_rate = max(R_E2E_v - R_e, 0)

        # Constraint C6: Γ_i^(s) >= Γ_s_req
        # The sensing rate is calculated if the sensing SNR meets the requirement.
        if snr_sense >= gamma_s_min:
            # Definition of sensing rate from the paper, following Eq. (24)
            sensing_rate = beta * B * np.log2(1 + max(snr_sense, snr_min))


        # This section calculates the sensing SNR at the eavesdropper, which is conceptually
        # related to Equation (24), which defines the SNR of the sensing signal at the eavesdropper.
        d_ge = abs(60 - eve_pos)
        pl_ge = compute_pathloss(d_ge)
        h_ge = np.array([np.exp(-1j * 2 * np.pi / lambda_c * n * d_ge) for n in range(N)]).reshape(1, -1)
        h_ge = h_ge / np.sqrt(pl_ge)

        theta = np.exp(1j * ris_action)
        Theta = np.diag(theta)

        # RIS-assisted echo path
        H_is = H_br
        f_ris_e = (h_ge @ Theta @ H_is).reshape(M, 1)

        # Direct path
        f_direct_e = (np.random.randn(M, 1) + 1j * np.random.randn(M, 1)) / np.sqrt(2)

        # Composite channel
        F1_e = f_direct_e @ f_direct_e.conj().T
        F2_e = f_ris_e @ f_ris_e.conj().T
        F_ge = F1_e + F2_e

        # Same transmit matrix as before
        P_tx = W_tau @ W_tau.conj().T + W_o @ W_o.conj().T
        sigma_e = 0.1  # same noise power

        # Radar receive vector
        u_e = np.random.randn(M, 1) + 1j * np.random.randn(M, 1)
        u_e = u_e / np.linalg.norm(u_e)

        snr_sense_eve = (u_e.conj().T @ F_ge @ P_tx @ F_ge.conj().T @ u_e).real / sigma_e**2

        R_sense_eve = beta * B * np.log2(1 + max(_scalar(snr_sense_eve), snr_min))
        R_sense_i = sensing_rate
        # Equation (25) - Sensing secrecy rate (S^(s)).
        # This is also related to Constraint C10: S^(s) > 0.
        sensing_secrecy_rate = max(R_sense_i - R_sense_eve, 0)

        if secrecy_rate == 0 and snr_comm >= gamma_req:
            R_v = beta * B * np.log2(1 + max(snr_comm, snr_min))
            R_e = beta * B * np.log2(1 + max(snr_eve, snr_min))
            C_D_i = beta * B * np.log2(1 + max(np.abs(h_backhaul)**2 / sigma2, snr_min))
            total_Rv = R_v * V
            R_D_v = R_v / total_Rv * C_D_i
            R_E2E_v = min(R_v, R_D_v)
            secrecy_rate = max(R_E2E_v - R_e, 0)

        else:
            R_v, R_e = 0, 0  # Define default values if condition fails


        sensing_rate = beta * B * np.log2(1 + max(snr_sense, snr_min))
        # Objective Function of Equation (26) - Maximize the weighted sum of secrecy and sensing rates.
        reward = omega * secrecy_rate + (1 - omega) * sensing_rate

        if snr_sense_eve > gamma_s_min:
            reward -= 1.5 * 1e6
#### Penalty and reward are to make outcome convergence faster and to get improved performance...
        # Soft penalty: stronger when eavesdropper receives better signal
        leakage_penalty = np.log2(1 + _scalar(snr_sense_eve)) / np.log2(1 + gamma_s_min)
        reward -= 1/ (1+leakage_penalty)

        if secrecy_rate > 0 and snr_sense_eve < gamma_s_min:
            reward += 2.0  # Strong positive reward when secure & private


        reward = np.clip(reward, 0, 10)


        agent.reward_history.append(reward)
        next_state_np = np.concatenate([ris_action, bs_action])

        if agent.is_text_based or agent.is_hybrid:
            next_prompt = create_prompt(next_state_np, {'secrecy': secrecy_rate, 'sensing': sensing_rate})
            agent.replay_buffer.push((prompt, noisy_action, [reward], next_prompt, state_np, next_state_np))
        else:
            agent.replay_buffer.push(state_np, noisy_action, [reward], next_state_np)

        agent.update(batch_size, gamma, tau, tokenizer)

    noise_std = max(noise_std * noise_decay, min_noise_std)

    if (ep + 1) % 100 == 0:
        print(f"Episode {ep + 1}/{episodes} | Noise: {noise_std:.3f}")
        for name, agent in agents.items():
            avg_reward = np.mean(agent.reward_history[-100:])
            print(f"  - {name}: Last 100 Avg Reward = {avg_reward:.4f}")
    if ep < 700:  # for first 5 episodes
        print(f"Ep{ep+1} | SNR_comm={snr_comm:.2e}, SNR_eve={snr_eve:.2e}, R_v={R_v:.4f}, R_e={R_e:.4f}, Secrecy={secrecy_rate:.4f}, Sensing={sensing_rate:.4f}, Reward={reward:.4f}")


print("Training finished.")

# --- 7. Save Data and Plot Results ---
print("Saving reward data to 'plots' directory...")
for name, agent in agents.items():
    np.save(f'plots/{name}_rewards.npy', agent.reward_history)

def moving_avg(x, k=50):
    return np.convolve(x, np.ones(k)/k, mode='valid')

def plot_comparison(save_path='plots/actor_comparison.png'):
    plt.figure(figsize=(12, 7))
    agent_names = ["MLP", "LLM", "Hybrid"]
    colors = ['#1f77b4', '#2ca02c', '#d62728'] # Blue, Green, Red

    for name, color in zip(agent_names, colors):
        try:
            rewards = np.load(f'plots/{name}_rewards.npy')
            plt.plot(moving_avg(rewards, k=100), label=f'DDPG-{name}', linewidth=2.5, color=color)
        except FileNotFoundError:
            print(f"Warning: 'plots/{name}_rewards.npy' not found. Skipping.")

    plt.xlabel('Episode', fontsize=14)
    plt.ylabel('Reward (Weighted Rate)', fontsize=14)
    plt.title('DDPG Actor Architecture Comparison (Secrecy vs. Sensing)', fontsize=16)
    plt.legend(fontsize=12)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"Comparison plot saved to '{save_path}'")
    plt.show()

# Generate the final comparison plot
plot_comparison()